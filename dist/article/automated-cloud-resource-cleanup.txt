1:"$Sreact.fragment"
2:I[95329,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"default"]
3:I[6724,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"default"]
5:I[68637,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[68637,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"ViewportBoundary"]
a:I[68637,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"MetadataBoundary"]
c:I[13248,[],"default"]
:HL["/_next/static/chunks/7c4046cfcd0c4728.css","style"]
:HL["/_next/static/media/07454f8ad8aaac57-s.p.fc65572f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/0c89a48fa5027cee-s.p.4564287c.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/70bc3e132a0a741e-s.p.15008bfb.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"uxEJewJWDoJ_apfxTgedq","c":["","article","automated-cloud-resource-cleanup"],"q":"","i":false,"f":[[["",{"children":["article",{"children":[["slug","automated-cloud-resource-cleanup","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/7c4046cfcd0c4728.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"space_grotesk_e6988195-module__RNs2Mq__variable nunito_faf8bdde-module__ZdPV6W__variable jetbrains_mono_7ea1d0f9-module__6GV5LG__variable antialiased","style":{"fontFamily":"var(--font-nunito)"},"children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","script","script-0",{"src":"/_next/static/chunks/0ae95c80abeb4b39.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$L9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$Lb"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:I[83207,["/_next/static/chunks/0ae95c80abeb4b39.js"],"default"]
e:Te5e9,<br/>The average cloud account has 30-40% waste. Here's how to find and eliminate it automatically.</p><p>In this guide, I'll show you how to identify zombie resources—unattached EBS volumes, old snapshots, idle instances, forgotten dev environments—and eliminate them with automated cleanup scripts. We'll cover manual audit techniques, Python automation, EventBridge scheduling, and open source tools like aws-nuke and Cloud Custodian.</p><p>Whether you're managing a single AWS account or dozens, these strategies will help you stop paying for resources you're not using.</p><p><h2>$2</h2></p><p>Cloud waste is silent, cumulative, and expensive. It doesn't happen overnight—it builds over months and years as teams spin up resources, forget them, and move on to the next project.</p><p><h3>The Numbers</h3></p><p>Let's start with the reality check:</p><p><li><strong>32% of cloud spend</strong> is wasted on unused resources (industry average)</li><br/><li><strong>$5.3 billion annually</strong> wasted on oversized resources alone</li><br/><li><strong>$22+ billion</strong> in global cloud waste per year</li><br/><li><strong>40% of instances</strong> run at less than 5% CPU utilization</li><br/><li><strong>Orphaned snapshots and volumes</strong> represent 1-3% of monthly spend without cleanup automation</li></p><p>These aren't theoretical numbers from white papers—they're from real audits of production environments.</p><p><h3>Real-World Examples</h3></p><p><strong>Case 1: 500 Unattached EBS Volumes</strong></p><p>A mid-sized startup had 500 EBS volumes with no attached instances:<br/><li>Volume types: 300 gp2, 200 io1</li><br/><li>Total size: 2.3 TB</li><br/><li>Monthly cost: $1,840</li></p><p>None of these volumes were attached to any instance. They were remnants from terminated instances that developers forgot to clean up.</p><p><strong>Case 2: 3,000 Old Snapshots</strong></p><p>An enterprise had accumulated 3,000 snapshots over 4 years:<br/><li>Average age: 180 days</li><br/><li>Total storage: 4.8 TB</li><br/><li>Monthly cost: $432</li><br/><li>Problem: Only 12 snapshots referenced by active AMIs</li></p><p>88% of snapshots had no purpose—they were just consuming storage and costing money.</p><p><strong>Case 3: 200 Unused Elastic IPs</strong></p><p>A development environment had 200 Elastic IPs not associated with any resource:<br/><li>Each EIP: $3.60/month</li><br/><li>Monthly cost: $720</li><br/><li>Problem: These addresses weren't even allocated to running instances</li></p><p><strong>Case 4: 87 Idle Instances</strong></p><p>A production environment had 87 EC2 instances running at <5% CPU for 30+ days:<br/><li>Instance types: Mix of m5.large, c5.xlarge, r5.2xlarge</li><br/><li>Monthly cost: $12,500</li><br/><li>Problem: These instances were ghost deployments from old projects</li></p><p><h3>Why Zombie Resources Accumulate</h3></p><p><strong>1. Terminated Instances ≠ Deleted Volumes</strong></p><p>When you terminate an EC2 instance, AWS gives you the option to delete attached volumes. If you don't explicitly select "Delete on Termination," the volume persists.</p><p><pre><code># Instance terminated<br/>aws ec2 terminate-instances --instance-ids i-0123456789abcdef0</p><p><h1>But the volume remains</h1><br/>aws ec2 describe-volumes --filters Name=attachment.instance-id,Values=i-0123456789abcdef0<br/><h1>Returns: The volume still exists, still costs money</h1><br/></code></pre></p><p><strong>2. Snapshot Sprawl</strong></p><p>Every backup creates a snapshot. Without retention policies:<br/><li>Automated backups create snapshots daily</li><br/><li>Manual snapshots before deployments</li><br/><li>Testing snapshots that are never deleted</li><br/><li>Old snapshots with no AMI reference</li></p><p><strong>3. Development Environments That Never Sleep</strong></p><p>Dev and staging environments often run 24/7 despite only being used during business hours:<br/><li>Usage pattern: 8 hours/day, 5 days/week</li><br/><li>Billing pattern: 24 hours/day, 7 days/week</li><br/><li>Waste: 67% of runtime cost</li></p><p><strong>4. Elastic IPs Without Purpose</strong></p><p>Elastic IPs cost $3.60/month even if not attached. Common causes:<br/><li>Released instances but kept the IP "just in case"</li><br/><li>Forgotten load balancers</li><br/><li>Test environments abandoned</li></p><p><strong>5. Oversized Instances</strong></p><p>Right-sizing issues:<br/><li>Initial deployment over-provisioning</li><br/><li>Workload changes over time</li><br/><li>Migration from on-prem (resources sized for physical hardware)</li><br/><li>"Better safe than sorry" mentality</li></p><p><h2>$2</h2></p><p>Before automating cleanup, you need to understand what you have. Start with a comprehensive manual audit.</p><p><h3>Step 1: Enable Cost Explorer</h3></p><p>Cost Explorer is your visibility tool—it's free and essential.</p><p><pre><code># Enable Cost Explorer (one-time setup)<br/>aws ce enable-ce<br/></code></pre></p><p>Once enabled, wait 24 hours for data to populate, then:</p><p><li>Go to AWS Console → Cost Management → Cost Explorer</li><br/><li>Create a "Monthly costs by service" view</li><br/><li>Create a "Daily costs" view</li><br/><li>Set up cost anomaly detection</li><br/><li>Create budgets for monitoring</li></p><p><h3>Step 2: Find Unattached EBS Volumes</h3></p><p><pre><code># Find all unattached EBS volumes in a region<br/>aws ec2 describe-volumes \<br/>  --filters Name=status,Values=available \<br/>  --query 'Volumes[?State==<code>available</code>].[VolumeId,Size,VolumeType,CreateTime]' \<br/>  --output table</p><p><h1>Calculate monthly cost of unattached volumes</h1><br/>aws ec2 describe-volumes \<br/>  --filters Name=status,Values=available \<br/>  --query 'Volumes[*].[VolumeId,Size,VolumeType]' \<br/>  --output json | jq -r '.[] | @csv' | while IFS=, read -r vid size type; do<br/>    # gp2 pricing: $0.10/GB/month in us-east-1<br/>    cost=$(echo "$size * 0.10" | bc)<br/>    echo "$vid,$size,$type,$\${cost}"<br/>  done<br/></code></pre></p><p><h3>Step 3: Find Old Snapshots</h3></p><p><pre><code># Find snapshots older than 90 days<br/>DATE=$(date -u -d '90 days ago' +%Y-%m-%dT%H:%M:%S)</p><p>aws ec2 describe-snapshots \<br/>  --owner-ids self \<br/>  --filters "Name=start-time,Values=,\${DATE}" \<br/>  --query 'Snapshots[*].[SnapshotId,VolumeSize,StartTime,Description]' \<br/>  --output table</p><p><h1>Find snapshots not referenced by any AMI</h1><br/>aws ec2 describe-snapshots \<br/>  --owner-ids self \<br/>  --query 'Snapshots[*].SnapshotId' \<br/>  --output text | while read snap_id; do<br/>    referenced=$(aws ec2 describe-images \<br/>      --filters Name=block-device-mapping.snapshot-id,Values=$snap_id \<br/>      --query 'Images | length(@)' \<br/>      --output text)<br/>    if [ "$referenced" == "0" ]; then<br/>      echo "$snap_id is not referenced by any AMI"<br/>    fi<br/>  done<br/></code></pre></p><p><h3>Step 4: Find Idle EC2 Instances</h3></p><p><pre><code># Get CPU utilization for all instances over 30 days<br/>aws cloudwatch get-metric-statistics \<br/>  --namespace AWS/EC2 \<br/>  --metric-name CPUUtilization \<br/>  --dimensions Name=InstanceId,Value=i-0123456789abcdef0 \<br/>  --start-time $(date -u -d '30 days ago' --iso-8601=seconds) \<br/>  --end-time $(date -u --iso-8601=seconds) \<br/>  --period 86400 \<br/>  --statistics Average \<br/>  --query 'Datapoints[0].Average'<br/></code></pre></p><p>Or use a Python script for all instances (see below).</p><p><h3>Step 5: Find Unused Elastic IPs</h3></p><p><pre><code># Find Elastic IPs not associated with any instance<br/>aws ec2 describe-addresses \<br/>  --query 'Addresses[?AssociationId==<code>null</code>].[PublicIp,AllocationId]' \<br/>  --output table</p><p><h1>Count and calculate cost</h1><br/>unused_eips=$(aws ec2 describe-addresses \<br/>  --query 'Addresses[?AssociationId==<code>null</code>] | length(@)' \<br/>  --output text)</p><p>monthly_cost=$(echo "$unused_eips * 3.60" | bc)<br/>echo "Unused EIPs: $unused_eips"<br/>echo "Monthly cost: $\${monthly_cost}"<br/></code></pre></p><p><h3>Step 6: Find Unused Lambda Versions</h3></p><p><pre><code># List all Lambda functions and their versions<br/>aws lambda list-functions \<br/>  --query 'Functions[*].[FunctionName,Version]' \<br/>  --output table</p><p><h1>Get all versions for a specific function</h1><br/>aws lambda list-versions-by-function \<br/>  --function-name my-function \<br/>  --query 'Versions[*].[Version,LastModified]' \<br/>  --output table<br/></code></pre></p><p><h2>$2</h2></p><p>Now let's build automation that finds and eliminates zombie resources. These scripts are production-ready with safety checks and dry-run modes.</p><p><h3>Script 1: Find and Delete Unattached EBS Volumes</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>EBS Volume Cleanup Script<br/>Finds and deletes unattached EBS volumes with safety checks.<br/>"""</p><p>import boto3<br/>from datetime import datetime, timedelta<br/>import sys</p><p>def get_unattached_volumes(dry_run=True):<br/>    """Get all volumes not attached to any instance."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    # Filter for available (unattached) volumes<br/>    response = ec2.describe_volumes(<br/>        Filters=[<br/>            {'Name': 'status', 'Values': ['available']}<br/>        ]<br/>    )<br/>    <br/>    volumes = response['Volumes']<br/>    print(""<br/>    <br/>    # Calculate total cost<br/>    total_size = sum(v['Size'] for v in volumes)<br/>    # gp2 pricing in us-east-1: $0.10/GB/month<br/>    monthly_cost = total_size * 0.10<br/>    <br/>    print("Total size: {} GB".format(total_size))<br/>    print("Monthly cost: ${:.2f}".format(monthly_cost))<br/>    <br/>    return volumes</p><p>def check_tags(volume):<br/>    """Check if volume has exemption tags."""<br/>    tags = volume.get('Tags', [])<br/>    tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>    <br/>    # Exempt if any of these tags exist<br/>    exempt_tags = ['do-not-delete', 'preserve', 'keep', 'exempt']<br/>    <br/>    for exempt_tag in exempt_tags:<br/>        if exempt_tag in tag_dict:<br/>            print(""<br/>            return True<br/>    <br/>    return False</p><p>def delete_volume(volume_id, dry_run=True):<br/>    """Delete a volume with safety checks."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    try:<br/>        if dry_run:<br/>            print(""<br/>        else:<br/>            ec2.delete_volume(VolumeId=volume_id)<br/>            print(""<br/>        return True<br/>    except Exception as e:<br/>        print(""<br/>        return False</p><p>def main():<br/>    dry_run = '--dry-run' in sys.argv or '-d' in sys.argv<br/>    force = '--force' in sys.argv or '-f' in sys.argv<br/>    <br/>    if dry_run:<br/>        print("=" * 60)<br/>        print("DRY RUN MODE - No changes will be made")<br/>        print("=" * 60)<br/>    else:<br/>        print("=" * 60)<br/>        print("PRODUCTION MODE - Volumes WILL be deleted!")<br/>        print("=" * 60)<br/>    <br/>    if not dry_run and not force:<br/>        confirm = input("\nThis will delete unattached volumes. Continue? (yes/no): ")<br/>        if confirm.lower() != 'yes':<br/>            print("Aborted.")<br/>            sys.exit(0)<br/>    <br/>    volumes = get_unattached_volumes(dry_run)<br/>    <br/>    if not volumes:<br/>        print("No unattached volumes found.")<br/>        return<br/>    <br/>    deleted_count = 0<br/>    exempt_count = 0<br/>    total_size_deleted = 0<br/>    <br/>    for volume in volumes:<br/>        volume_id = volume['VolumeId']<br/>        size = volume['Size']<br/>        create_time = volume['CreateTime']<br/>        age_days = (datetime.now(create_time.tzinfo) - create_time).days<br/>        <br/>        print(""<br/>        <br/>        # Check exemption tags<br/>        if check_tags(volume):<br/>            exempt_count += 1<br/>            continue<br/>        <br/>        # Additional safety: volumes created in last 7 days<br/>        if age_days < 7:<br/>            print(""<br/>            exempt_count += 1<br/>            continue<br/>        <br/>        # Delete the volume<br/>        if delete_volume(volume_id, dry_run):<br/>            deleted_count += 1<br/>            total_size_deleted += size<br/>    <br/>    print("\n" + "=" * 60)<br/>    print(""<br/>    print(""<br/>    print(""<br/>    print(""</p><p>if __name__ == '__main__':<br/>    main()<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Dry run (safe, see what would be deleted)<br/>python3 cleanup_ebs.py --dry-run</p><p><h1>Production run (with confirmation)</h1><br/>python3 cleanup_ebs.py</p><p><h1>Production run (force, no confirmation)</h1><br/>python3 cleanup_ebs.py --force<br/></code></pre></p><p><h3>Script 2: Clean Old Snapshots</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>Snapshot Cleanup Script<br/>Deletes old snapshots not referenced by any AMI.<br/>"""</p><p>import boto3<br/>from datetime import datetime, timedelta, timezone<br/>import sys</p><p>def get_old_snapshots(days=90, dry_run=True):<br/>    """Get snapshots older than specified days."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)<br/>    <br/>    response = ec2.describe_snapshots(<br/>        OwnerIds=['self'],<br/>        Filters=[<br/>            {'Name': 'start-time', 'Values': [f',\{cutoff_date.isoformat()}']}<br/>        ]<br/>    )<br/>    <br/>    snapshots = response['Snapshots']<br/>    print(""<br/>    <br/>    return snapshots</p><p>def is_snapshot_referenced(snapshot_id):<br/>    """Check if snapshot is referenced by any AMI."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    response = ec2.describe_images(<br/>        Filters=[<br/>            {'Name': 'block-device-mapping.snapshot-id', 'Values': [snapshot_id]}<br/>        ]<br/>    )<br/>    <br/>    return len(response['Images']) > 0</p><p>def delete_snapshot(snapshot_id, dry_run=True):<br/>    """Delete a snapshot."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    try:<br/>        if dry_run:<br/>            print(""<br/>        else:<br/>            ec2.delete_snapshot(SnapshotId=snapshot_id)<br/>            print(""<br/>        return True<br/>    except Exception as e:<br/>        print(""<br/>        return False</p><p>def main():<br/>    dry_run = '--dry-run' in sys.argv or '-d' in sys.argv<br/>    force = '--force' in sys.argv or '-f' in sys.argv<br/>    days = int(sys.argv[sys.argv.index('--days') + 1]) if '--days' in sys.argv else 90<br/>    <br/>    if dry_run:<br/>        print("=" * 60)<br/>        print(""<br/>        print(""<br/>        print("=" * 60)<br/>    else:<br/>        print("=" * 60)<br/>        print(""<br/>        print(""<br/>        print("=" * 60)<br/>    <br/>    if not dry_run and not force:<br/>        confirm = input("\nThis will delete old unreferenced snapshots. Continue? (yes/no): ")<br/>        if confirm.lower() != 'yes':<br/>            print("Aborted.")<br/>            sys.exit(0)<br/>    <br/>    snapshots = get_old_snapshots(days, dry_run)<br/>    <br/>    if not snapshots:<br/>        print("No old snapshots found.")<br/>        return<br/>    <br/>    deleted_count = 0<br/>    exempt_count = 0<br/>    total_size_deleted = 0<br/>    <br/>    for snapshot in snapshots:<br/>        snapshot_id = snapshot['SnapshotId']<br/>        size = snapshot['VolumeSize']<br/>        start_time = snapshot['StartTime']<br/>        description = snapshot.get('Description', '')<br/>        <br/>        age_days = (datetime.now(timezone.utc) - start_time).days<br/>        <br/>        print(""<br/>        if description:<br/>            print(""<br/>        <br/>        # Check if referenced by AMI<br/>        if is_snapshot_referenced(snapshot_id):<br/>            print(""<br/>            exempt_count += 1<br/>            continue<br/>        <br/>        # Check exemption tags<br/>        tags = snapshot.get('Tags', [])<br/>        tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>        <br/>        exempt_tags = ['do-not-delete', 'preserve', 'keep', 'backup']<br/>        for exempt_tag in exempt_tags:<br/>            if exempt_tag in tag_dict:<br/>                print(""<br/>                exempt_count += 1<br/>                continue<br/>        <br/>        # Delete the snapshot<br/>        if delete_snapshot(snapshot_id, dry_run):<br/>            deleted_count += 1<br/>            total_size_deleted += size<br/>    <br/>    print("\n" + "=" * 60)<br/>    print(""<br/>    print(""<br/>    print(""<br/>    # Snapshot pricing in us-east-1: $0.05/GB/month<br/>    print(""</p><p>if __name__ == '__main__':<br/>    main()<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Dry run for snapshots older than 90 days<br/>python3 cleanup_snapshots.py --dry-run --days 90</p><p><h1>Delete snapshots older than 180 days</h1><br/>python3 cleanup_snapshots.py --days 180</p><p><h1>Force delete (no confirmation)</h1><br/>python3 cleanup_snapshots.py --days 90 --force<br/></code></pre></p><p><h3>Script 3: Stop Idle EC2 Instances</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>Idle EC2 Instance Cleanup<br/>Stops instances with CPU < 5% for 7+ days.<br/>"""</p><p>import boto3<br/>from datetime import datetime, timedelta, timezone<br/>import sys</p><p>def get_instance_cpu(instance_id, days=7):<br/>    """Get average CPU utilization for an instance."""<br/>    cloudwatch = boto3.client('cloudwatch')<br/>    <br/>    end_time = datetime.now(timezone.utc)<br/>    start_time = end_time - timedelta(days=days)<br/>    <br/>    response = cloudwatch.get_metric_statistics(<br/>        Namespace='AWS/EC2',<br/>        MetricName='CPUUtilization',<br/>        Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],<br/>        StartTime=start_time,<br/>        EndTime=end_time,<br/>        Period=86400,  # Daily<br/>        Statistics=['Average']<br/>    )<br/>    <br/>    datapoints = response['Datapoints']<br/>    if not datapoints:<br/>        return None<br/>    <br/>    avg_cpu = sum(dp['Average'] for dp in datapoints) / len(datapoints)<br/>    return avg_cpu</p><p>def check_instance_tags(instance):<br/>    """Check if instance has exemption tags."""<br/>    tags = instance.get('Tags', [])<br/>    tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>    <br/>    exempt_tags = ['do-not-stop', 'always-on', 'production', 'critical']<br/>    <br/>    for exempt_tag in exempt_tags:<br/>        if exempt_tag.lower() in [k.lower() for k in tag_dict.keys()]:<br/>            print(""<br/>            return True<br/>    <br/>    return False</p><p>def stop_instance(instance_id, dry_run=True):<br/>    """Stop an EC2 instance."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    try:<br/>        if dry_run:<br/>            print(""<br/>        else:<br/>            ec2.stop_instances(InstanceIds=[instance_id])<br/>            print(""<br/>        return True<br/>    except Exception as e:<br/>        print(""<br/>        return False</p><p>def main():<br/>    dry_run = '--dry-run' in sys.argv or '-d' in sys.argv<br/>    force = '--force' in sys.argv or '-f' in sys.argv<br/>    cpu_threshold = float(sys.argv[sys.argv.index('--cpu') + 1]) if '--cpu' in sys.argv else 5.0<br/>    days = int(sys.argv[sys.argv.index('--days') + 1]) if '--days' in sys.argv else 7<br/>    <br/>    ec2 = boto3.client('ec2')<br/>    <br/>    # Get all running instances<br/>    response = ec2.describe_instances(<br/>        Filters=[<br/>            {'Name': 'instance-state-name', 'Values': ['running']}<br/>        ]<br/>    )<br/>    <br/>    instances = []<br/>    for reservation in response['Reservations']:<br/>        instances.extend(reservation['Instances'])<br/>    <br/>    print(""<br/>    <br/>    if dry_run:<br/>        print("=" * 60)<br/>        print("DRY RUN MODE - No instances will be stopped")<br/>        print(""<br/>        print("=" * 60)<br/>    else:<br/>        print("=" * 60)<br/>        print("PRODUCTION MODE - Instances WILL be stopped!")<br/>        print(""<br/>        print("=" * 60)<br/>    <br/>    if not dry_run and not force:<br/>        confirm = input("\nThis will stop idle instances. Continue? (yes/no): ")<br/>        if confirm.lower() != 'yes':<br/>            print("Aborted.")<br/>            sys.exit(0)<br/>    <br/>    stopped_count = 0<br/>    exempt_count = 0<br/>    <br/>    for instance in instances:<br/>        instance_id = instance['InstanceId']<br/>        instance_type = instance['InstanceType']<br/>        launch_time = instance['LaunchTime']<br/>        <br/>        # Skip instances launched in last 30 days<br/>        age_days = (datetime.now(timezone.utc) - launch_time).days<br/>        if age_days < 30:<br/>            continue<br/>        <br/>        # Get CPU utilization<br/>        avg_cpu = get_instance_cpu(instance_id, days)<br/>        <br/>        if avg_cpu is None:<br/>            print(""<br/>            print(""<br/>            continue<br/>        <br/>        print(""<br/>        print(""<br/>        <br/>        if avg_cpu >= cpu_threshold:<br/>            print(""<br/>            continue<br/>        <br/>        # Check exemption tags<br/>        if check_instance_tags(instance):<br/>            exempt_count += 1<br/>            continue<br/>        <br/>        # Stop the instance<br/>        if stop_instance(instance_id, dry_run):<br/>            stopped_count += 1<br/>    <br/>    print("\n" + "=" * 60)<br/>    print(""<br/>    print(""<br/>    print(""</p><p>if __name__ == '__main__':<br/>    main()<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Dry run: find instances with CPU < 5% for 7 days<br/>python3 cleanup_instances.py --dry-run --cpu 5.0 --days 7</p><p><h1>Stop instances with CPU < 3% for 14 days</h1><br/>python3 cleanup_instances.py --cpu 3.0 --days 14</p><p><h1>Force stop (no confirmation)</h1><br/>python3 cleanup_instances.py --cpu 5.0 --days 7 --force<br/></code></pre></p><p><h3>Script 4: Delete Unused Elastic IPs</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>Elastic IP Cleanup Script<br/>Releases Elastic IPs not associated with any resource.<br/>"""</p><p>import boto3<br/>import sys</p><p>def get_unused_eips(dry_run=True):<br/>    """Get Elastic IPs not associated with any instance."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    response = ec2.describe_addresses()<br/>    <br/>    unused_eips = [<br/>        addr for addr in response['Addresses']<br/>        if addr.get('AssociationId') is None<br/>    ]<br/>    <br/>    print(""<br/>    print(""<br/>    <br/>    return unused_eips</p><p>def check_tags(address):<br/>    """Check if address has exemption tags."""<br/>    tags = address.get('Tags', [])<br/>    tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>    <br/>    exempt_tags = ['do-not-delete', 'preserve', 'keep']<br/>    <br/>    for exempt_tag in exempt_tags:<br/>        if exempt_tag in tag_dict:<br/>            print(""<br/>            return True<br/>    <br/>    return False</p><p>def release_eip(allocation_id, dry_run=True):<br/>    """Release an Elastic IP."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    try:<br/>        if dry_run:<br/>            print(""<br/>        else:<br/>            ec2.release_address(AllocationId=allocation_id)<br/>            print(""<br/>        return True<br/>    except Exception as e:<br/>        print(""<br/>        return False</p><p>def main():<br/>    dry_run = '--dry-run' in sys.argv or '-d' in sys.argv<br/>    force = '--force' in sys.argv or '-f' in sys.argv<br/>    <br/>    if dry_run:<br/>        print("=" * 60)<br/>        print("DRY RUN MODE - No EIPs will be released")<br/>        print("=" * 60)<br/>    else:<br/>        print("=" * 60)<br/>        print("PRODUCTION MODE - EIPs WILL be released!")<br/>        print("=" * 60)<br/>    <br/>    if not dry_run and not force:<br/>        confirm = input("\nThis will release unused Elastic IPs. Continue? (yes/no): ")<br/>        if confirm.lower() != 'yes':<br/>            print("Aborted.")<br/>            sys.exit(0)<br/>    <br/>    eips = get_unused_eips(dry_run)<br/>    <br/>    if not eips:<br/>        print("No unused Elastic IPs found.")<br/>        return<br/>    <br/>    released_count = 0<br/>    exempt_count = 0<br/>    <br/>    for addr in eips:<br/>        public_ip = addr['PublicIp']<br/>        allocation_id = addr['AllocationId']<br/>        <br/>        print(""<br/>        <br/>        # Check exemption tags<br/>        if check_tags(addr):<br/>            exempt_count += 1<br/>            continue<br/>        <br/>        # Release the address<br/>        if release_eip(allocation_id, dry_run):<br/>            released_count += 1<br/>    <br/>    print("\n" + "=" * 60)<br/>    print(""<br/>    print(""<br/>    print(""<br/>    print(""</p><p>if __name__ == '__main__':<br/>    main()<br/></code></pre></p><p><h3>Script 5: Clean Old Lambda Versions</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>Lambda Version Cleanup Script<br/>Deletes old versions of Lambda functions.<br/>"""</p><p>import boto3<br/>from datetime import datetime, timedelta, timezone<br/>import sys</p><p>def get_lambda_versions(function_name, keep_versions=5, dry_run=True):<br/>    """Get old versions of a Lambda function."""<br/>    lambda_client = boto3.client('lambda')<br/>    <br/>    response = lambda_client.list_versions_by_function(<br/>        FunctionName=function_name<br/>    )<br/>    <br/>    versions = response['Versions']<br/>    <br/>    # Sort by last modified (newest first)<br/>    versions.sort(key=lambda v: v.get('LastModified', datetime.now(timezone.utc)), reverse=True)<br/>    <br/>    # Keep the latest versions<br/>    versions_to_delete = versions[keep_versions:]<br/>    <br/>    if versions_to_delete:<br/>        print(""<br/>    <br/>    return versions_to_delete</p><p>def delete_lambda_version(function_name, version, dry_run=True):<br/>    """Delete a specific Lambda version."""<br/>    lambda_client = boto3.client('lambda')<br/>    <br/>    try:<br/>        if dry_run:<br/>            print(""<br/>        else:<br/>            lambda_client.delete_function(<br/>                FunctionName=function_name,<br/>                Qualifier=version<br/>            )<br/>            print(""<br/>        return True<br/>    except Exception as e:<br/>        print(""<br/>        return False</p><p>def main():<br/>    dry_run = '--dry-run' in sys.argv or '-d' in sys.argv<br/>    force = '--force' in sys.argv or '-f' in sys.argv<br/>    keep_versions = int(sys.argv[sys.argv.index('--keep') + 1]) if '--keep' in sys.argv else 5<br/>    <br/>    lambda_client = boto3.client('lambda')<br/>    <br/>    # Get all Lambda functions<br/>    response = lambda_client.list_functions()<br/>    <br/>    functions = [func['FunctionName'] for func in response['Functions']]<br/>    print(""<br/>    <br/>    if dry_run:<br/>        print("=" * 60)<br/>        print("DRY RUN MODE - No versions will be deleted")<br/>        print(""<br/>        print("=" * 60)<br/>    else:<br/>        print("=" * 60)<br/>        print("PRODUCTION MODE - Versions WILL be deleted!")<br/>        print(""<br/>        print("=" * 60)<br/>    <br/>    if not dry_run and not force:<br/>        confirm = input("\nThis will delete old Lambda versions. Continue? (yes/no): ")<br/>        if confirm.lower() != 'yes':<br/>            print("Aborted.")<br/>            sys.exit(0)<br/>    <br/>    deleted_count = 0<br/>    <br/>    for function_name in functions:<br/>        old_versions = get_lambda_versions(function_name, keep_versions, dry_run)<br/>        <br/>        for version in old_versions:<br/>            version_num = version['Version']<br/>            <br/>            # Never delete $LATEST<br/>            if version_num == '$LATEST':<br/>                continue<br/>            <br/>            if delete_lambda_version(function_name, version_num, dry_run):<br/>                deleted_count += 1<br/>    <br/>    print("\n" + "=" * 60)<br/>    print(""<br/>    print(""</p><p>if __name__ == '__main__':<br/>    main()<br/></code></pre></p><p><h2>$2</h2></p><p>Manual scripts are great, but automated is better. Let's set up scheduled cleanup using EventBridge and Lambda.</p><p><h3>Step 1: Create IAM Role for Lambda</h3></p><p><pre><code># Create IAM policy for cleanup<br/>cat > cleanup-policy.json <<'EOF'<br/>{<br/>  "Version": "2012-10-17",<br/>  "Statement": [<br/>    {<br/>      "Effect": "Allow",<br/>      "Action": [<br/>        "ec2:DescribeVolumes",<br/>        "ec2:DeleteVolume",<br/>        "ec2:DescribeSnapshots",<br/>        "ec2:DeleteSnapshot",<br/>        "ec2:DescribeInstances",<br/>        "ec2:StopInstances",<br/>        "ec2:DescribeAddresses",<br/>        "ec2:ReleaseAddress",<br/>        "cloudwatch:GetMetricStatistics"<br/>      ],<br/>      "Resource": "*"<br/>    }<br/>  ]<br/>}<br/>EOF</p><p><h1>Create policy</h1><br/>aws iam create-policy \<br/>  --policy-name LambdaCleanupPolicy \<br/>  --policy-document file://cleanup-policy.json</p><p><h1>Get policy ARN</h1><br/>POLICY_ARN=$(aws iam list-policies \<br/>  --query "Policies[?PolicyName=='LambdaCleanupPolicy'].Arn" \<br/>  --output text)</p><p><h1>Create IAM role</h1><br/>cat > trust-policy.json <<'EOF'<br/>{<br/>  "Version": "2012-10-17",<br/>  "Statement": [<br/>    {<br/>      "Effect": "Allow",<br/>      "Principal": {<br/>        "Service": "lambda.amazonaws.com"<br/>      },<br/>      "Action": "sts:AssumeRole"<br/>    }<br/>  ]<br/>}<br/>EOF</p><p>aws iam create-role \<br/>  --role-name LambdaCleanupRole \<br/>  --assume-role-policy-document file://trust-policy.json</p><p><h1>Attach policy to role</h1><br/>aws iam attach-role-policy \<br/>  --role-name LambdaCleanupRole \<br/>  --policy-arn $POLICY_ARN<br/></code></pre></p><p><h3>Step 2: Create Lambda Function</h3></p><p><pre><code># lambda_handler.py<br/>import json<br/>import boto3<br/>from datetime import datetime, timedelta, timezone</p><p>def lambda_handler(event, context):<br/>    """Lambda handler for scheduled cleanup."""<br/>    print(""<br/>    <br/>    # Get configuration from event<br/>    config = event.get('config', {})<br/>    <br/>    results = {<br/>        'timestamp': datetime.now(timezone.utc).isoformat(),<br/>        'cleanup_tasks': []<br/>    }<br/>    <br/>    # EBS Volume Cleanup<br/>    if config.get('cleanup_ebs', False):<br/>        result = cleanup_ebs(dry_run=config.get('dry_run', False))<br/>        results['cleanup_tasks'].append({<br/>            'task': 'cleanup_ebs',<br/>            'result': result<br/>        })<br/>    <br/>    # Snapshot Cleanup<br/>    if config.get('cleanup_snapshots', False):<br/>        result = cleanup_snapshots(<br/>            days=config.get('snapshot_days', 90),<br/>            dry_run=config.get('dry_run', False)<br/>        )<br/>        results['cleanup_tasks'].append({<br/>            'task': 'cleanup_snapshots',<br/>            'result': result<br/>        })<br/>    <br/>    # EC2 Instance Cleanup<br/>    if config.get('cleanup_instances', False):<br/>        result = cleanup_instances(<br/>            cpu_threshold=config.get('cpu_threshold', 5.0),<br/>            days=config.get('instance_days', 7),<br/>            dry_run=config.get('dry_run', False)<br/>        )<br/>        results['cleanup_tasks'].append({<br/>            'task': 'cleanup_instances',<br/>            'result': result<br/>        })<br/>    <br/>    print(""<br/>    <br/>    return {<br/>        'statusCode': 200,<br/>        'body': json.dumps(results)<br/>    }</p><p>def cleanup_ebs(dry_run=True):<br/>    """Cleanup unattached EBS volumes."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    response = ec2.describe_volumes(<br/>        Filters=[{'Name': 'status', 'Values': ['available']}]<br/>    )<br/>    <br/>    volumes = response['Volumes']<br/>    deleted = 0<br/>    exempted = 0<br/>    size_deleted = 0<br/>    <br/>    for volume in volumes:<br/>        volume_id = volume['VolumeId']<br/>        tags = volume.get('Tags', [])<br/>        tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>        <br/>        # Check exemption tags<br/>        if any(tag in tag_dict for tag in ['do-not-delete', 'preserve', 'keep']):<br/>            exempted += 1<br/>            continue<br/>        <br/>        try:<br/>            if not dry_run:<br/>                ec2.delete_volume(VolumeId=volume_id)<br/>            deleted += 1<br/>            size_deleted += volume['Size']<br/>        except Exception as e:<br/>            print(""<br/>    <br/>    return {<br/>        'deleted': deleted,<br/>        'exempted': exempted,<br/>        'size_gb': size_deleted<br/>    }</p><p>def cleanup_snapshots(days=90, dry_run=True):<br/>    """Cleanup old unreferenced snapshots."""<br/>    ec2 = boto3.client('ec2')<br/>    <br/>    cutoff = datetime.now(timezone.utc) - timedelta(days=days)<br/>    <br/>    response = ec2.describe_snapshots(<br/>        OwnerIds=['self'],<br/>        Filters=[<br/>            {'Name': 'start-time', 'Values': [f',\{cutoff.isoformat()}']}<br/>        ]<br/>    )<br/>    <br/>    snapshots = response['Snapshots']<br/>    deleted = 0<br/>    exempted = 0<br/>    size_deleted = 0<br/>    <br/>    for snapshot in snapshots:<br/>        snapshot_id = snapshot['SnapshotId']<br/>        <br/>        # Check if referenced by AMI<br/>        ami_response = ec2.describe_images(<br/>            Filters=[<br/>                {'Name': 'block-device-mapping.snapshot-id', 'Values': [snapshot_id]}<br/>            ]<br/>        )<br/>        <br/>        if len(ami_response['Images']) > 0:<br/>            exempted += 1<br/>            continue<br/>        <br/>        try:<br/>            if not dry_run:<br/>                ec2.delete_snapshot(SnapshotId=snapshot_id)<br/>            deleted += 1<br/>            size_deleted += snapshot['VolumeSize']<br/>        except Exception as e:<br/>            print(""<br/>    <br/>    return {<br/>        'deleted': deleted,<br/>        'exempted': exempted,<br/>        'size_gb': size_deleted<br/>    }</p><p>def cleanup_instances(cpu_threshold=5.0, days=7, dry_run=True):<br/>    """Cleanup idle EC2 instances."""<br/>    ec2 = boto3.client('ec2')<br/>    cloudwatch = boto3.client('cloudwatch')<br/>    <br/>    response = ec2.describe_instances(<br/>        Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]<br/>    )<br/>    <br/>    instances = []<br/>    for reservation in response['Reservations']:<br/>        instances.extend(reservation['Instances'])<br/>    <br/>    stopped = 0<br/>    exempted = 0<br/>    <br/>    for instance in instances:<br/>        instance_id = instance['InstanceId']<br/>        <br/>        # Check exemption tags<br/>        tags = instance.get('Tags', [])<br/>        tag_dict = {tag['Key']: tag['Value'] for tag in tags}<br/>        <br/>        if any(tag.lower() in [k.lower() for k in tag_dict.keys()] <br/>               for tag in ['do-not-stop', 'always-on', 'production']):<br/>            exempted += 1<br/>            continue<br/>        <br/>        # Get CPU<br/>        end_time = datetime.now(timezone.utc)<br/>        start_time = end_time - timedelta(days=days)<br/>        <br/>        metrics_response = cloudwatch.get_metric_statistics(<br/>            Namespace='AWS/EC2',<br/>            MetricName='CPUUtilization',<br/>            Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],<br/>            StartTime=start_time,<br/>            EndTime=end_time,<br/>            Period=86400,<br/>            Statistics=['Average']<br/>        )<br/>        <br/>        datapoints = metrics_response['Datapoints']<br/>        if not datapoints:<br/>            continue<br/>        <br/>        avg_cpu = sum(dp['Average'] for dp in datapoints) / len(datapoints)<br/>        <br/>        if avg_cpu < cpu_threshold:<br/>            try:<br/>                if not dry_run:<br/>                    ec2.stop_instances(InstanceIds=[instance_id])<br/>                stopped += 1<br/>            except Exception as e:<br/>                print(""<br/>    <br/>    return {<br/>        'stopped': stopped,<br/>        'exempted': exempted<br/>    }<br/></code></pre></p><p><h3>Step 3: Package and Deploy Lambda</h3></p><p><pre><code># Create deployment package<br/>zip lambda_cleanup.zip lambda_handler.py</p><p><h1>Create Lambda function</h1><br/>aws lambda create-function \<br/>  --function-name cloud-cleanup \<br/>  --runtime python3.11 \<br/>  --role arn:aws:iam::123456789012:role/LambdaCleanupRole \<br/>  --handler lambda_handler.lambda_handler \<br/>  --zip-file fileb://lambda_cleanup.zip \<br/>  --timeout 300 \<br/>  --memory-size 512</p><p><h1>Update function (for later updates)</h1><br/>aws lambda update-function-code \<br/>  --function-name cloud-cleanup \<br/>  --zip-file fileb://lambda_cleanup.zip<br/></code></pre></p><p><h3>Step 4: Create EventBridge Rule</h3></p><p><pre><code># Create EventBridge rule (weekly cleanup, Monday 2 AM UTC)<br/>aws events put-rule \<br/>  --name weekly-cloud-cleanup \<br/>  --schedule-expression 'cron(0 2 ? * MON *)' \<br/>  --description 'Weekly cloud resource cleanup'</p><p><h1>Add Lambda as target</h1><br/>aws lambda add-permission \<br/>  --function-name cloud-cleanup \<br/>  --statement-id events-invoke \<br/>  --action lambda:InvokeFunction \<br/>  --principal events.amazonaws.com \<br/>  --source-arn arn:aws:events:us-east-1:123456789012:rule/weekly-cloud-cleanup</p><p>aws events put-targets \<br/>  --rule weekly-cloud-cleanup \<br/>  --targets '{<br/>    "Id": "1",<br/>    "Arn": "arn:aws:lambda:us-east-1:123456789012:function:cloud-cleanup",<br/>    "Input": "{<br/>      \"config\": {<br/>        \"cleanup_ebs\": true,<br/>        \"cleanup_snapshots\": true,<br/>        \"cleanup_instances\": true,<br/>        \"dry_run\": false,<br/>        \"snapshot_days\": 90,<br/>        \"cpu_threshold\": 5.0,<br/>        \"instance_days\": 7<br/>      }<br/>    }"<br/>  }'<br/></code></pre></p><p><h3>Step 5: Test Lambda Invocation</h3></p><p><pre><code># Test invocation (dry run)<br/>aws lambda invoke \<br/>  --function-name cloud-cleanup \<br/>  --payload '{<br/>    "config": {<br/>      "cleanup_ebs": true,<br/>      "cleanup_snapshots": true,<br/>      "cleanup_instances": true,<br/>      "dry_run": true<br/>    }<br/>  }' \<br/>  response.json</p><p><h1>View logs</h1><br/>aws logs tail /aws/lambda/cloud-cleanup --follow<br/></code></pre></p><p><h2>$2</h2></p><p>Custom scripts are powerful, but mature open source tools offer more features and community support.</p><p><h3>aws-nuke</h3></p><p><strong>aws-nuke</strong> is a tool for cleaning up AWS accounts by nuking (deleting) all resources. It's designed for cleaning up test accounts or entire environments.</p><p><strong>Installation:</strong></p><p><pre><code># Download latest release<br/>curl -L https://github.com/rebuy-de/aws-nuke/releases/download/v3.30.0/aws-nuke-v3.30.0-linux-amd64.tar.gz | tar xz</p><p><h1>Move to PATH</h1><br/>sudo mv aws-nuke-v3.30.0-linux-amd64 /usr/local/bin/aws-nuke<br/></code></pre></p><p><strong>Configuration:</strong></p><p><pre><code># config.yaml<br/>regions:<br/>  - us-east-1<br/>  - us-west-2</p><p>accounts:<br/>  123456789012:<br/>    filters:<br/>      # Only delete unattached EBS volumes<br/>      EC2Volume:<br/>        - property: State<br/>          value: available<br/>      # Only delete snapshots older than 90 days<br/>      EC2Snapshot:<br/>        - property: StartDate<br/>          type: older_than<br/>          value: "90d"<br/>      # Never delete production resources<br/>      EC2Instance:<br/>        - or:<br/>            - property: tag:Environment<br/>              value: production<br/>            - property: tag:do-not-delete<br/>              value: "true"<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Dry run (always do this first!)<br/>aws-nuke run --config config.yaml --dry-run</p><p><h1>Actual nuke (be very careful!)</h1><br/>aws-nuke run --config config.yaml --no-dry-run<br/></code></pre></p><p><strong>Pros:</strong><br/><li>Extremely fast and thorough</li><br/><li>Powerful filter system</li><br/><li>Excellent for cleaning up test/dev environments</li></p><p><strong>Cons:</strong><br/><li>Can be dangerous in production</li><br/><li>Requires careful configuration</li><br/><li>Not selective enough for production cleanup</li></p><p><h3>Cloud Custodian</h3></p><p><strong>Cloud Custodian</strong> is a rules engine for managing AWS resources, including automated cleanup. It's more policy-driven than aws-nuke.</p><p><strong>Installation:</strong></p><p><pre><code># Install via pip<br/>pip install custodian</p><p><h1>Or via Homebrew (Mac)</h1><br/>brew install cloud-custodian<br/></code></pre></p><p><strong>Configuration:</strong></p><p><pre><code># cleanup.yml<br/>policies:<br/>  - name: delete-unattached-ebs<br/>    resource: ebs<br/>    filters:<br/>      - type: value<br/>        key: State<br/>        value: available<br/>      - not:<br/>          - type: value<br/>            key: "tag:do-not-delete"<br/>            value: "true"<br/>    actions:<br/>      - type: delete<br/>        # Require confirmation via email<br/>        notify:<br/>          - type: email<br/>            to:<br/>              - ops@company.com</p><p>  - name: delete-old-snapshots<br/>    resource: ebs-snapshot<br/>    filters:<br/>      - type: value<br/>        key: "tag:Age"<br/>        op: greater-than<br/>        value: 90<br/>      - not:<br/>          - type: value<br/>            key: "tag:do-not-delete"<br/>            value: "true"<br/>    actions:<br/>      - type: delete</p><p>  - name: stop-idle-instances<br/>    resource: ec2<br/>    filters:<br/>      - type: metrics<br/>        name: CPUUtilization<br/>        days: 7<br/>        value: 5<br/>        op: less-than<br/>      - not:<br/>          - type: value<br/>            key: "tag:do-not-stop"<br/>            value: "true"<br/>    actions:<br/>      - type: stop</p><p>  - name: release-unused-eips<br/>    resource: network-addr<br/>    filters:<br/>      - type: value<br/>        key: AssociationId<br/>        value: null<br/>      - not:<br/>          - type: value<br/>            key: "tag:do-not-delete"<br/>            value: "true"<br/>    actions:<br/>      - type: release<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Dry run<br/>custodian run cleanup.yml --dry-run</p><p><h1>Run with output to S3 for logging</h1><br/>custodian run cleanup.yml \<br/>  --s3-bucket custodian-logs \<br/>  --region us-east-1</p><p><h1>Schedule with Lambda</h1><br/>custodian run cleanup.yml \<br/>  --output-dir s3://custodian-logs/reports \<br/>  --region us-east-1 \<br/>  --lambda<br/></code></pre></p><p><strong>Pros:</strong><br/><li>Policy-driven, rules-based approach</li><br/><li>Supports multiple clouds (AWS, Azure, GCP)</li><br/><li>Rich filter and action ecosystem</li><br/><li>Can be scheduled with Lambda</li><br/><li>Good for ongoing governance</li></p><p><strong>Cons:</strong><br/><li>YAML configuration can be complex</li><br/><li>Steeper learning curve than simple scripts</li><br/><li>Requires more setup for basic cleanup</li></p><p><h3>Komiser</h3></p><p><strong>Komiser</strong> is a cloud cost monitoring and resource governance tool. It's less about cleanup and more about visibility, but it can help identify waste.</p><p><strong>Installation:</strong></p><p><pre><code># Docker<br/>docker run -d -p 3000:3000 \<br/>  -v $HOME/.komiser:/app/.komiser \<br/>  komiser/komiser:latest</p><p><h1>Or via Homebrew</h1><br/>brew tap komiserhq/komiser<br/>brew install komiser<br/></code></pre></p><p><strong>Configuration:</strong></p><p><pre><code># komiser.yaml<br/>credentials:<br/>  - name: aws-prod<br/>    type: aws<br/>    id: AKIA...<br/>    secret: ...<br/>    source: account<br/>    account: 123456789012</p><p>  - name: gcp-prod<br/>    type: gcp<br/>    source: account<br/>    credentials: /path/to/service-account.json<br/></code></pre></p><p><strong>Usage:</strong></p><p><pre><code># Start dashboard<br/>komiser start</p><p><h1>List all resources</h1><br/>komiser resources --provider aws</p><p><h1>Get cost by service</h1><br/>komiser costs --provider aws --by service</p><p><h1>Get cost by region</h1><br/>komiser costs --provider aws --by region<br/></code></pre></p><p><strong>Komiser Web Dashboard:</strong><br/><li>View all resources across clouds</li><br/><li>Filter by provider, region, service</li><br/><li>Track costs in real-time</li><br/><li>Identify underutilized resources</li></p><p><strong>Pros:</strong><br/><li>Multi-cloud support</li><br/><li>Beautiful web dashboard</li><br/><li>Good for visibility first, cleanup later</li><br/><li>Real-time cost tracking</li></p><p><strong>Cons:</strong><br/><li>Less automated cleanup than Cloud Custodian</li><br/><li>More of a monitoring tool than a cleanup tool</li><br/><li>Requires manual action on findings</li></p><p><h2>$2</h2></p><p>Automated cleanup without safety is dangerous. Here's how to protect production resources.</p><p><h3>Tagging Strategy</h3></p><p><strong>Required Tags:</strong></p><p><pre><code># Environment tag<br/>Environment: production|staging|development</p><p><h1>Owner tag</h1><br/>Owner: team-name or individual-email</p><p><h1>Purpose tag</h1><br/>Purpose: what-the-resource-does</p><p><h1>Exemption tags</h1><br/>do-not-delete: "true"<br/>do-not-stop: "true"<br/>keep: "true"<br/>preserve: "true"<br/></code></pre></p><p><strong>Tag Enforcement:</strong></p><p><pre><code># Cloud Custodian policy to enforce tagging<br/>policies:<br/>  - name: enforce-resource-tagging<br/>    resource: ec2<br/>    filters:<br/>      - or:<br/>          - type: value<br/>            key: "tag:Environment"<br/>            value: absent<br/>          - type: value<br/>            key: "tag:Owner"<br/>            value: absent<br/>          - type: value<br/>            key: "tag:Purpose"<br/>            value: absent<br/>    actions:<br/>      - type: notify<br/>        subject: "Untagged EC2 instance detected"<br/>        to:<br/>          - ops@company.com<br/>      - type: mark-for-op<br/>        op: stop<br/>        days: 7<br/></code></pre></p><p><h3>Dry-Run Mode</h3></p><p>Always implement dry-run mode in your scripts:</p><p><pre><code># Pattern for dry-run<br/>def delete_resource(resource_id, dry_run=True):<br/>    """Delete a resource with dry-run mode."""<br/>    if dry_run:<br/>        print(""<br/>        return False<br/>    else:<br/>        # Actual deletion<br/>        try:<br/>            client.delete(ResourceId=resource_id)<br/>            print(""<br/>            return True<br/>        except Exception as e:<br/>            print(""<br/>            return False</p><p><h1>Require explicit confirmation</h1><br/>if not dry_run:<br/>    confirm = input("This will delete resources. Type 'yes' to confirm: ")<br/>    if confirm.lower() != 'yes':<br/>        sys.exit(0)<br/></code></pre></p><p><h3>Approval Workflows</h3></p><p>For production environments, use approval workflows:</p><p><pre><code># Cloud Custodian with approval<br/>policies:<br/>  - name: delete-unattached-volumes-with-approval<br/>    resource: ebs<br/>    filters:<br/>      - type: value<br/>        key: State<br/>        value: available<br/>    actions:<br/>      - type: notify<br/>        subject: "Unattached EBS volumes found - Approve deletion?"<br/>        template: approval-email.j2<br/>        to:<br/>          - ops@company.com<br/>        transport:<br/>          type: sns<br/>          topic: arn:aws:sns:us-east-1:123456789012:cleanup-approvals<br/></code></pre></p><p>When recipients reply with "APPROVE", a Lambda function triggers the actual deletion.</p><p><h2>$2</h2></p><p>Measure your savings to prove ROI.</p><p><h3>Before Cleanup</h3></p><p><pre><code># Get current monthly cost estimate<br/>aws ce get-cost-and-usage \<br/>  --time-period Start=$(date -u -d '30 days ago' --iso-8601=seconds),End=$(date -u --iso-8601=seconds) \<br/>  --granularity MONTHLY \<br/>  --metrics UnblendedCost \<br/>  --query 'ResultsByTime[0].Total.UnblendedCost' \<br/>  --output text</p><p><h1>Get cost by service</h1><br/>aws ce get-cost-and-usage \<br/>  --time-period Start=$(date -u -d '30 days ago' --iso-8601=seconds),End=$(date -u --iso-8601=seconds) \<br/>  --granularity MONTHLY \<br/>  --metrics UnblendedCost \<br/>  --group-by Type=DIMENSION,Key=SERVICE<br/></code></pre></p><p><h3>After Cleanup</h3></p><p><pre><code># Wait 30 days for data to settle<br/><h1>Then run same queries</h1></p><p><h1>Calculate savings</h1><br/>BEFORE_COST=$(echo "$cost_before")<br/>AFTER_COST=$(echo "$cost_after")<br/>SAVINGS=$(echo "$BEFORE_COST - $AFTER_COST" | bc)<br/>PERCENT=$(echo "$SAVINGS / $BEFORE_COST * 100" | bc -l)</p><p>echo "Before: $\${BEFORE_COST}/month"<br/>echo "After: $\${AFTER_COST}/month"<br/>echo "Savings: $\${SAVINGS}/month (${PERCENT:.1f}%)"<br/></code></pre></p><p><h3>Tracking Script</h3></p><p><pre><code>#!/usr/bin/env python3<br/>"""<br/>Track cost savings from cleanup efforts.<br/>"""</p><p>import boto3<br/>from datetime import datetime, timedelta<br/>import json</p><p>def get_monthly_cost(days=30):<br/>    """Get monthly cost for the last N days."""<br/>    ce = boto3.client('ce')<br/>    <br/>    end_date = datetime.now().strftime('%Y-%m-%d')<br/>    start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')<br/>    <br/>    response = ce.get_cost_and_usage(<br/>        TimePeriod={<br/>            'Start': start_date,<br/>            'End': end_date<br/>        },<br/>        Granularity='MONTHLY',<br/>        Metrics=['UnblendedCost']<br/>    )<br/>    <br/>    return float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])</p><p>def track_cleanup_results():<br/>    """Track and log cleanup results."""<br/>    <br/>    # Load previous costs<br/>    try:<br/>        with open('cleanup_cost_history.json', 'r') as f:<br/>            history = json.load(f)<br/>    except FileNotFoundError:<br/>        history = {'entries': []}<br/>    <br/>    # Get current cost<br/>    current_cost = get_monthly_cost(30)<br/>    <br/>    # Calculate savings<br/>    if history['entries']:<br/>        previous_cost = history['entries'][0]['cost']<br/>        savings = previous_cost - current_cost<br/>        savings_percent = (savings / previous_cost) * 100 if previous_cost > 0 else 0<br/>    else:<br/>        savings = 0<br/>        savings_percent = 0<br/>    <br/>    # Add entry<br/>    entry = {<br/>        'date': datetime.now().isoformat(),<br/>        'cost': current_cost,<br/>        'savings': savings,<br/>        'savings_percent': savings_percent<br/>    }<br/>    history['entries'].insert(0, entry)<br/>    <br/>    # Keep last 12 entries<br/>    history['entries'] = history['entries'][:12]<br/>    <br/>    # Save<br/>    with open('cleanup_cost_history.json', 'w') as f:<br/>        json.dump(history, f, indent=2)<br/>    <br/>    # Print summary<br/>    print(""<br/>    if savings > 0:<br/>        print(""<br/>    <br/>    return entry</p><p>if __name__ == '__main__':<br/>    track_cleanup_results()<br/></code></pre></p><p><h2>$2</h2></p><p>Here are specific savings I've seen from implementing automated cleanup.</p><p><h3>Example 1: Startup with 50 EC2 Instances</h3></p><p><strong>Before:</strong><br/><li>Monthly cost: $8,500</li><br/><li>Issues: 12 idle instances, 30 unattached volumes</li><br/><li>Identified: 38% waste</li></p><p><strong>After cleanup:</strong><br/><li>Stopped 12 idle instances: $2,400/month saved</li><br/><li>Deleted 30 volumes: $600/month saved</li><br/><li>Total savings: $3,000/month (35%)</li></p><p><strong>ROI:</strong><br/><li>Setup time: 16 hours</li><br/><li>Infrastructure cost: $20/month (Lambda + EventBridge)</li><br/><li>Monthly savings: $3,000</li><br/><li>Payback: < 1 week</li></p><p><h3>Example 2: Enterprise with 200 EBS Volumes</h3></p><p><strong>Before:</strong><br/><li>Monthly cost: $15,200</li><br/><li>Issues: 87 unattached volumes (3.2 TB)</li><br/><li>Identified: 21% waste</li></p><p><strong>After cleanup:</strong><br/><li>Deleted 87 unattached volumes: $1,280/month saved</li><br/><li>Implemented 90-day snapshot retention: $450/month saved</li><br/><li>Total savings: $1,730/month (11.4%)</li></p><p><strong>ROI:</strong><br/><li>Setup time: 24 hours</li><br/><li>Infrastructure cost: $30/month</li><br/><li>Monthly savings: $1,730</li><br/><li>Payback: 1 week</li></p><p><h3>Example 3: Dev Environment Auto-Shutdown</h3></p><p><strong>Before:</strong><br/><li>Monthly cost: $4,800</li><br/><li>Issues: Dev runs 24/7, used 8 hours/day</li><br/><li>Identified: 67% runtime waste</li></p><p><strong>After auto-shutdown:</strong><br/><li>Dev shuts down 7 PM - 7 AM: $3,200/month saved</li><br/><li>Weekends off: $1,000/month saved</li><br/><li>Total savings: $4,200/month (87.5%)</li></p><p><strong>ROI:</strong><br/><li>Setup time: 8 hours</li><br/><li>Infrastructure cost: $15/month (Lambda + EventBridge)</li><br/><li>Monthly savings: $4,200</li><br/><li>Payback: < 1 day</li></p><p><h3>Example 4: Snapshot Sprawl Cleanup</h3></p><p><strong>Before:</strong><br/><li>Monthly cost: $22,000</li><br/><li>Issues: 3,500 snapshots, only 150 referenced by AMIs</li><br/><li>Identified: 2.1% waste (but significant in absolute terms)</li></p><p><strong>After cleanup:</strong><br/><li>Deleted 3,350 unreferenced snapshots: $637/month saved</li><br/><li>Implemented 30-day retention: $843/month saved</li><br/><li>Total savings: $1,480/month (6.7%)</li></p><p><strong>ROI:</strong><br/><li>Setup time: 12 hours</li><br/><li>Infrastructure cost: $25/month</li><br/><li>Monthly savings: $1,480</li><br/><li>Payback: 3 days</li></p><p><h2>$2</h2></p><p><h3>Phase 1: Quick Wins (Week 1)</h3></p><p><li>[ ] Enable Cost Explorer</li><br/><li>[ ] Run manual audit of all regions</li><br/><li>[ ] Delete unattached EBS volumes (dry run → confirm → delete)</li><br/><li>[ ] Release unused Elastic IPs</li><br/><li>[ ] Clean old snapshots (>180 days)</li></p><p><strong>Expected savings:</strong> 10-20%</p><p><h3>Phase 2: Automation (Weeks 2-3)</h3></p><p><li>[ ] Implement tagging strategy</li><br/><li>[ ] Deploy EBS cleanup script with dry-run mode</li><br/><li>[ ] Deploy snapshot cleanup script</li><br/><li>[ ] Set up Slack notifications for review</li></p><p><strong>Expected savings:</strong> Additional 5-10%</p><p><h3>Phase 3: Scheduling (Week 4)</h3></p><p><li>[ ] Package cleanup scripts as Lambda functions</li><br/><li>[ ] Create EventBridge rules for weekly execution</li><br/><li>[ ] Implement cost tracking dashboard</li><br/><li>[ ] Set up approval workflow for production</li></p><p><strong>Expected savings:</strong> Ongoing, automatic</p><p><h3>Phase 4: Advanced (Month 2)</h3></p><p><li>[ ] Deploy Cloud Custodian for policy-driven cleanup</li><br/><li>[ ] Set up dev environment auto-shutdown</li><br/><li>[ ] Implement right-sizing recommendations</li><br/><li>[ ] Create FinOps dashboard</li></p><p><strong>Expected savings:</strong> 20-30% total reduction</p><p><h2>$2</h2></p><p><h3>Pitfall 1: Deleting Production Resources</h3></p><p><strong>Problem:</strong> Automated cleanup deletes production resources because they weren't tagged.</p><p><strong>Solution:</strong><br/><li>Implement mandatory tagging</li><br/><li>Use do-not-delete/do-not-stop exemption tags</li><br/><li>Always run dry-run first</li><br/><li>Require approval for production environments</li></p><p><h3>Pitfall 2: Deleting Recently Created Resources</h3></p><p><strong>Problem:</strong> Resources created for testing are deleted before they're deployed.</p><p><strong>Solution:</strong><br/><li>Add age-based filters (e.g., skip resources < 7 days old)</li><br/><li>Use separate cleanup schedules for dev and prod</li><br/><li>Implement staging environment</li></p><p><h3>Pitfall 3: Not Checking Dependencies</h3></p><p><strong>Problem:</strong> Deleting a volume or snapshot breaks an AMI or running instance.</p><p><strong>Solution:</strong><br/><li>Always check AMI references before deleting snapshots</li><br/><li>Verify volumes aren't attached before deletion</li><br/><li>Use Cloud Custodian's dependency checking</li></p><p><h3>Pitfall 4: No Rollback Plan</h3></p><p><strong>Problem:</strong> Something goes wrong and there's no way to recover.</p><p><strong>Solution:</strong><br/><li>Keep backups of critical snapshots</li><br/><li>Document all cleanup actions</li><br/><li>Use CloudTrail for audit logs</li><br/><li>Have a rollback plan for critical resources</li></p><p><h3>Pitfall 5: Not Communicating with Teams</h3></p><p><strong>Problem:</strong> Engineers are surprised when their resources disappear.</p><p><strong>Solution:</strong><br/><li>Send notification emails before cleanup</li><br/><li>Use Slack notifications</li><br/><li>Create a cleanup review process</li><br/><li>Document cleanup policies</li></p><p><h2>$2</h2></p><p>Cloud waste is real, expensive, and cumulative. But it's also solvable.</p><p>The average cloud account wastes 30-40% of its spend on resources that aren't being used. That's not a rounding error—it's millions of dollars annually for mid-sized organizations.</p><p>By implementing automated cleanup, you can:</p><p><li><strong>Eliminate zombie resources</strong>—Unattached volumes, old snapshots, idle instances</li><br/><li><strong>Stop paying for what you don't use</strong>—Dev environments that run 24/7 but are only used 8 hours/day</li><br/><li><strong>Build continuous optimization</strong>—Weekly cleanup that prevents waste from accumulating</li><br/><li><strong>Prove ROI</strong>—Real cost savings you can show to leadership</li></p><p><strong>Key takeaways:</strong></p><p><li><strong>Start with visibility</strong>—Enable Cost Explorer and run a manual audit</li><br/><li><strong>Implement safety first</strong>—Tagging strategy, dry-run mode, approval workflows</li><br/><li><strong>Automate early</strong>—Python scripts → Lambda → EventBridge</li><br/><li><strong>Measure everything</strong>—Track before/after costs to prove ROI</li><br/><li><strong>Iterate continuously</strong>—Cleanup is never "done," it's a process</li></p><p><strong>Your next steps:</strong></p><p><li>Run the manual audit scripts (today)</li><br/><li>Implement the EBS cleanup script (this week)</li><br/><li>Deploy Lambda automation (this month)</li><br/><li>Build toward continuous optimization (ongoing)</li></p><p>Stop paying for what you're not using. Your cloud bill will thank you.</p><p>Built by engineers, for engineers.<br/>    4:[["$","$Ld",null,{}],["$","article",null,{"style":{"padding":"10rem 2rem 6rem","maxWidth":"900px","margin":"0 auto","position":"relative","zIndex":1,"animation":"fadeInUp 0.9s ease-out 0.2s both"},"children":[["$","header",null,{"style":{"marginBottom":"4rem"},"children":[["$","h1",null,{"style":{"fontFamily":"var(--font-space-grotesk)","fontSize":"clamp(2rem, 5vw, 3rem)","fontWeight":700,"lineHeight":1.2,"marginBottom":"1.5rem","color":"var(--text-primary)"},"children":"Automated Cloud Resource Cleanup: Stop Paying for What You're Not Using"}],["$","p",null,{"style":{"fontSize":"1.15rem","color":"var(--text-secondary)","marginBottom":"1.5rem","lineHeight":1.8},"children":"The average cloud account has 30-40% waste. Learn how to find and eliminate zombie resources automatically with Python, Bash, and open source tools."}],["$","div",null,{"style":{"display":"flex","alignItems":"center","gap":"1rem","fontSize":"0.85rem","color":"var(--text-muted)","fontFamily":"var(--font-jetbrains-mono)"},"children":[["$","span",null,{"style":{"padding":"0.4rem 1rem","background":"rgba(0, 212, 255, 0.1)","color":"var(--accent-cyan)","border":"1px solid rgba(0, 212, 255, 0.2)","borderRadius":"20px"},"children":["20 min"," read"]}],["$","span",null,{"style":{"padding":"0.4rem 1rem","background":"rgba(168, 85, 247, 0.1)","color":"var(--accent-purple)","border":"1px solid rgba(168, 85, 247, 0.2)","borderRadius":"20px"},"children":"Cloud Cost Optimization"}]]}]]}],["$","div",null,{"style":{"lineHeight":1.8,"color":"var(--text-primary)","fontSize":"1.05rem"},"dangerouslySetInnerHTML":{"__html":"$e"}}],"$Lf"]}],"$L10"]
f:["$","div",null,{"style":{"marginTop":"5rem","maxWidth":"600px","marginLeft":"auto","marginRight":"auto","background":"var(--bg-card)","border":"1px solid var(--border-subtle)","borderRadius":"24px","padding":"3.5rem","position":"relative","overflow":"hidden","boxShadow":"var(--card-shadow)"},"children":[["$","div",null,{"style":{"position":"absolute","top":"-2px","left":"-2px","right":"-2px","bottom":"-2px","background":"linear-gradient(135deg, var(--accent-cyan), var(--accent-purple), var(--accent-cyan))","borderRadius":"26px","zIndex":-1,"opacity":0.3}}],["$","h2",null,{"style":{"fontFamily":"var(--font-space-grotesk)","fontSize":"2rem","fontWeight":700,"marginBottom":"0.75rem","color":"var(--text-primary)","position":"relative","zIndex":1},"children":"Get weekly cloud cost tips"}],["$","p",null,{"style":{"color":"var(--text-secondary)","marginBottom":"2rem","fontSize":"1rem","lineHeight":1.8,"position":"relative","zIndex":1},"children":"Join engineers saving money on cloud costs. Actionable strategies every Friday."}],["$","a",null,{"href":"https://sendfox.com/form/3qdz96/36enr2","style":{"display":"inline-block","fontFamily":"var(--font-nunito)","fontSize":"1rem","fontWeight":700,"padding":"1.2rem 2rem","background":"linear-gradient(135deg, var(--accent-cyan) 0%, var(--accent-purple) 100%)","color":"white","border":"none","borderRadius":"12px","cursor":"pointer","textDecoration":"none","transition":"all 0.3s ease","position":"relative","zIndex":1},"children":"Subscribe (free)"}]]}]
10:["$","footer",null,{"style":{"padding":"3rem 2rem","textAlign":"center","borderTop":"1px solid var(--border-subtle)","position":"relative","zIndex":1,"animation":"fadeInUp 0.9s ease-out 0.4s both"},"children":["$","p",null,{"style":{"fontFamily":"var(--font-nunito)","fontSize":"0.85rem","color":"var(--text-muted)"},"children":"© 2026 Cost Nimbus. Built by engineers, for engineers."}]}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
11:I[64481,["/_next/static/chunks/d6bee75b35a28280.js","/_next/static/chunks/8189c34e001333f7.js"],"IconMark"]
7:null
b:[["$","title","0",{"children":"Automated Cloud Resource Cleanup: Stop Paying for What You're Not Using | Cost Nimbus | Cost Nimbus"}],["$","meta","1",{"name":"description","content":"The average cloud account has 30-40% waste. Learn how to find and eliminate zombie resources automatically with Python, Bash, and open source tools."}],["$","meta","2",{"name":"keywords","content":"Cloud Cost Optimization, cloud costs, AWS optimization, FinOps, cloud savings, cost management"}],["$","meta","3",{"property":"og:title","content":"Cost Nimbus - Cloud Cost Intelligence"}],["$","meta","4",{"property":"og:description","content":"Real cloud cost optimization strategies built by engineers, for engineers. Save thousands monthly with battle-tested techniques."}],["$","meta","5",{"property":"og:site_name","content":"Cost Nimbus"}],["$","meta","6",{"property":"og:type","content":"website"}],["$","meta","7",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","8",{"name":"twitter:title","content":"Cost Nimbus - Cloud Cost Intelligence"}],["$","meta","9",{"name":"twitter:description","content":"Real cloud cost optimization strategies built by engineers, for engineers. Save thousands monthly with battle-tested techniques."}],["$","link","10",{"rel":"icon","href":"/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L11","11",{}]]
